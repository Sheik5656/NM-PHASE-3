{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1xu_kNN6Ijl7ZJwOIzb5iGoXwPE_mCAiz"},"id":"GMJVYdrx4x60","outputId":"3996a0ac-d7d9-464a-de69-747cf784371d","executionInfo":{"status":"error","timestamp":1747245257857,"user_tz":-330,"elapsed":337431,"user":{"displayName":"SHeik Nijamudhin","userId":"03079502163923258677"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Install Libraries (If needed - Colab usually has these)\n","# !pip install pandas numpy matplotlib seaborn scikit-learn\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n","from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.utils import resample  # For handling class imbalance (if needed)\n","\n","# 1. Data Loading\n","try:\n","    df = pd.read_csv(\"telco_churn.csv\")  # Or the path to your dataset\n","except FileNotFoundError:\n","    print(\"Error: Dataset file not found. Please ensure the file is in the correct location or provide the correct path.\")\n","    # Instead of exit(), raise the exception to allow handling in notebook environment\n","    raise\n","\n","# 2. Data Cleaning\n","\n","## Missing Values\n","print(df.isnull().sum())   # Check for missing values\n","# Strategy:\n","# The 'TotalCharges' column will be converted to numeric. Non-numeric values will be coerced to NaN.\n","# These NaN values will then be filled with the median of the 'TotalCharges' column.\n","\n","# Convert 'TotalCharges' to numeric, handling potential errors\n","df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n","# 'errors='coerce'' will turn any non-numeric values into NaN (missing)\n","# Now fill the NaN values in 'TotalCharges' with the median\n","df['TotalCharges'] = df['TotalCharges'].fillna(df['TotalCharges'].median())\n","\n","## Duplicates\n","print(df.duplicated().sum())  # Check for duplicates\n","# Strategy:\n","# Duplicate rows will be identified and removed to ensure data integrity.\n","df = df.drop_duplicates()\n","\n","## Data Types\n","print(df.info())  # Check data types\n","# Strategy:\n","# The data types of the columns will be reviewed to ensure they are appropriate for analysis and modeling.\n","# 'TotalCharges' has already been converted to numeric.\n","\n","# 3. Exploratory Data Analysis (EDA)\n","\n","## Univariate Analysis\n","# Histograms for numerical features\n","df.hist(figsize=(12, 10))\n","plt.show()\n","# Insight: The histograms provide a visual distribution of the numerical features like 'tenure', 'MonthlyCharges', and 'TotalCharges'. 'tenure' seems to have a concentration at the lower end and a peak at the higher end, suggesting many new customers and many long-term customers. 'MonthlyCharges' has a wider distribution, and 'TotalCharges' is skewed to the right, which is expected as it's a product of 'tenure' and 'MonthlyCharges'.\n","\n","# Countplots for categorical features\n","for col in df.select_dtypes(include='object').columns:\n","    sns.countplot(data=df, x=col)\n","    plt.xticks(rotation=45, ha='right')\n","    plt.title(f'Countplot of {col}')\n","    plt.tight_layout()\n","    plt.show()\n","# Insights: These countplots show the distribution of each categorical feature. For example, we can see the number of customers for each 'InternetService' type, 'Contract' type, 'PaymentMethod', etc. The 'Churn' countplot shows the imbalance in the target variable, with more customers not churning than churning.\n","\n","# Boxplots for numerical features (outlier detection)\n","plt.figure(figsize=(10, 6))\n","sns.boxplot(data=df, x='Churn', y='tenure')  # Corrected column name to 'tenure'\n","plt.title('Boxplot of Tenure vs Churn')\n","plt.show()\n","\n","plt.figure(figsize=(10, 6))\n","sns.boxplot(data=df, x='Churn', y='MonthlyCharges')\n","plt.title('Boxplot of Monthly Charges vs Churn')\n","plt.show()\n","\n","plt.figure(figsize=(10, 6))\n","sns.boxplot(data=df, x='Churn', y='TotalCharges')\n","plt.title('Boxplot of Total Charges vs Churn')\n","plt.show()\n","# Insights: These boxplots help visualize the relationship between numerical features and the target variable 'Churn'. For instance, customers with shorter tenures and higher monthly charges seem more likely to churn. 'TotalCharges' for churned customers appears to have a wider spread but generally lower values, likely due to the shorter tenure.\n","\n","## Bivariate/Multivariate Analysis\n","# Correlation matrix\n","corr_matrix = df.corr(numeric_only=True)\n","plt.figure(figsize=(12, 8))\n","sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n","plt.title('Correlation Matrix')\n","plt.show()\n","# Insight: The correlation matrix shows the linear relationships between numerical features. 'tenure' and 'TotalCharges' have a strong positive correlation, which is expected. 'MonthlyCharges' also shows a positive correlation with 'TotalCharges', but weaker than 'tenure'. The correlation between 'Churn' and other numerical features is relatively weak, but negative for 'tenure' and 'TotalCharges', and slightly positive for 'MonthlyCharges'.\n","\n","# Scatterplots\n","sns.scatterplot(data=df, x='tenure', y='MonthlyCharges', hue='Churn')\n","plt.title('Scatterplot of Tenure vs Monthly Charges (colored by Churn)')\n","plt.show()\n","# Insight: This scatterplot shows that customers with high monthly charges and shorter tenures are more likely to churn. Customers with longer tenures tend to have a wider range of monthly charges and are less likely to churn.\n","\n","# Grouped bar plots\n","categorical_cols = df.select_dtypes(include='object').columns\n","for col in categorical_cols:\n","    sns.catplot(data=df, x='Churn', kind='count', hue=col, aspect=1, height=4)\n","    plt.title(f'Churn Distribution by {col}')\n","    plt.show()\n","# Insights: These grouped bar plots illustrate the distribution of churn across different categories within each categorical feature. For example, we can observe churn rates for different contract types, internet services, and payment methods. Month-to-month contracts appear to have a higher churn rate compared to one-year or two-year contracts. Customers with fiber optic internet service also seem to have a higher churn rate.\n","\n","# 4. Feature Engineering\n","\n","## Create New Features\n","df['AvgMonthlyUsage'] = df['TotalCharges'] / df['tenure']\n","# Rationale: This feature represents the average amount a customer pays per month during their tenure. It might provide insights into spending habits and their relation to churn.\n","\n","df['HasMultipleServices'] = ((df['PhoneService'] == 'Yes') & (df['InternetService'] == 'Yes') & (df['OnlineSecurity'] == 'Yes')).astype(int)\n","# Rationale: This binary feature indicates whether a customer subscribes to phone, internet, and online security services. Customers with a broader range of services might be less likely to churn.\n","\n","## Binning\n","df['Tenure_Group'] = pd.qcut(df['tenure'], q=4, labels=['0-1 Year', '1-2 Years', '2-5 Years', '5+ Years'])\n","# Strategy: The 'tenure' column is binned into four quartiles to group customers based on their service duration. This can help capture non-linear relationships between tenure and churn.\n","\n","## One-Hot Encode Tenure Group (and other categorical features if needed after binning)\n","df = pd.get_dummies(df, columns=['Tenure_Group'], drop_first=True)\n","\n","## Preprocessing for Modeling\n","# Identify numerical and categorical features\n","numerical_features = df.select_dtypes(include=np.number).drop(columns=['Churn']).columns.tolist()\n","categorical_features = df.select_dtypes(include='object').columns.tolist()\n","\n","# Create transformers for preprocessing\n","numerical_transformer = StandardScaler()\n","categorical_transformer = OneHotEncoder(handle_unknown='ignore', drop='first')  # handle_unknown='ignore' is important!\n","\n","# Combine transformers using ColumnTransformer\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numerical_transformer, numerical_features),\n","        ('cat', categorical_transformer, categorical_features)\n","    ],\n","    remainder='passthrough'  # Keep remaining columns (like engineered features)\n",")\n","\n","# 5. Model Building\n","\n","## Prepare Data\n","X = df.drop('Churn', axis=1)\n","y = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)  # Ensure y is numeric\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)  # Stratify!\n","\n","## Model 1: Logistic Regression\n","pipeline_lr = Pipeline(steps=[('preprocessor', preprocessor),\n","                      ('classifier', LogisticRegression(random_state=42, solver='liblinear'))])  # solver important for smaller datasets\n","pipeline_lr.fit(X_train, y_train)\n","y_pred_lr = pipeline_lr.predict(X_test)\n","y_pred_proba_lr = pipeline_lr.predict_proba(X_test)[:, 1]\n","\n","## Model 2: Random Forest\n","pipeline_rf = Pipeline(steps=[('preprocessor', preprocessor),\n","                      ('classifier', RandomForestClassifier(random_state=42))])\n","pipeline_rf.fit(X_train, y_train)\n","y_pred_rf = pipeline_rf.predict(X_test)\n","y_pred_proba_rf = pipeline_rf.predict_proba(X_test)[:, 1]\n","\n","## Model Evaluation\n","def evaluate_model(y_true, y_pred, y_pred_proba, model_name):\n","    print(f\"--- {model_name} ---\")\n","    print(f\"    Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n","    print(f\"    Precision: {precision_score(y_true, y_pred):.4f}\")\n","    print(f\"    Recall: {recall_score(y_true, y_pred):.4f}\")\n","    print(f\"    F1-score: {f1_score(y_true, y_pred):.4f}\")\n","    print(f\"    ROC-AUC: {roc_auc_score(y_true, y_pred_proba):.4f}\")\n","\n","    # Confusion Matrix\n","    cm = confusion_matrix(y_true, y_pred)\n","    plt.figure(figsize=(6, 5))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","    plt.title(f'Confusion Matrix - {model_name}')\n","    plt.ylabel('True Label')\n","    plt.xlabel('Predicted Label')\n","    plt.show()\n","\n","    # ROC Curve\n","    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n","    plt.figure(figsize=(6, 5))\n","    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc_score(y_true, y_pred_proba):.4f})')\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(f'ROC Curve - {model_name}')\n","    plt.legend()\n","    plt.show()\n","\n","evaluate_model(y_test, y_pred_lr, y_pred_proba_lr, 'Logistic Regression')\n","evaluate_model(y_test, y_pred_rf, y_pred_proba_rf, 'Random Forest')\n","\n","## Feature Importance (Random Forest)\n","feature_importances = pipeline_rf.named_steps['classifier'].feature_importances_\n","# Get feature names after preprocessing\n","feature_names = preprocessor.get_feature_names_out()\n","\n","# Extract the feature names corresponding to the original columns\n","original_feature_names = []\n","for name in feature_names:\n","    if 'num__' in name:\n","        original_feature_names.append(name.replace('num__', ''))\n","    elif 'cat__' in name:\n","        original_feature_names.append(name.replace('cat__', ''))\n","    else:\n","        original_feature_names.append(name) # For remainder='passthrough' features\n","\n","# Match feature importances to original feature names (handling one-hot encoded features)\n","importance_dict = {}\n","num_numerical = len(numerical_features)\n","num_categorical = len(categorical_features)\n","\n","for i in range(num_numerical):\n","    importance_dict[numerical_features[i]] = feature_importances[i]\n","\n","cat_start_index = num_numerical\n","for i in range(num_categorical):\n","    base_name = categorical_features[i]\n","    # Get the number of categories (excluding the dropped one)\n","    num_cats = len(pipeline_rf.named_steps['preprocessor'].transformers_[1][1].categories_[i]) -1\n","    for j in range(num_cats):\n","        importance_dict[f'{base_name}_{pipeline_rf.named_steps[\"preprocessor\"].transformers_[1][1].categories_[i][j+1]}'] = feature_importances[cat_start_index + i * (len(pipeline_rf.named_steps['preprocessor'].transformers_[1][1].categories_[i]) -1) + j]\n","\n","# Add importance for engineered features (assuming they are at the end after 'passthrough')\n","engineered_features = ['AvgMonthlyUsage', 'HasMultipleServices', 'Tenure_Group_1-2 Years', 'Tenure_Group_2-5 Years', 'Tenure_Group_5+ Years']\n","start_engineered_index = num_numerical + sum(len(pipeline_rf.named_steps['preprocessor'].transformers_[1][1].categories_[i]) - 1 for i in range(num_categorical))\n","\n","for i, feature in enumerate(engineered_features):\n","    if start_engineered_index + i < len(feature_importances):\n","        importance_dict[feature] = feature_importances[start_engineered_index + i]\n","\n","sorted_importance = sorted(importance_dict.items(), key=lambda item: item[1], reverse=True)\n","\n","plt.figure(figsize=(10, 10))\n","sns.barplot(x=[importance for name, importance in sorted_importance], y=[name for name, importance in sorted_importance])\n","plt.title('Feature Importance - Random Forest')\n","plt.xlabel('Importance')\n","plt.ylabel('Feature')\n","plt.tight_layout()\n","plt.show()\n","\n","# 6. Conclusion\n","# Summarize your findings, compare models, and discuss limitations and future work\n","print(\"\\n--- Conclusion ---\")\n","print(\"We built two classification models, Logistic Regression and Random Forest, to predict customer churn based on the Telco dataset.\")\n","print(\"\\nModel Performance:\")\n","print(\"Logistic Regression:\")\n","print(f\"  Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n","print(f\"  ROC-AUC: {roc_auc_score(y_test, y_pred_proba_lr):.4f}\")\n","print(\"\\nRandom Forest:\")\n","print(f\"  Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n","print(f\"  ROC-AUC: {roc_auc_score(y_test, y_pred_proba_rf):.4f}\")\n","\n","print(\"\\nObservations:\")\n","print(\"- Both models show reasonable performance in predicting churn.\")\n","print(\"- The Random Forest model generally exhibits slightly better performance, particularly in terms of accuracy and potentially in capturing non-linear relationships in the data, as suggested by the feature importance.\")\n","print(\"- Features like 'Contract_Month-to-month', 'tenure', 'MonthlyCharges', and 'TotalCharges' appear to be important predictors of churn based on the Random Forest feature importance.\")\n","print(\"- The ROC-AUC scores indicate the ability of both models to distinguish between churned and non-churned customers.\")\n","\n","print(\"\\nLimitations:\")\n","print(\"- The dataset might have class imbalance, which could affect model performance, especially recall and precision for the minority class (churned customers). We used `stratify` during the train-test split to mitigate this, but further techniques like oversampling or undersampling could be explored.\")\n","print(\"- The models are based on the features available in the dataset. Other external factors not included here could also influence churn.\")\n","print(\"- The performance of the models could potentially be improved by further hyperparameter tuning.\")\n","\n","print(\"\\nFuture Work:\")\n","print(\"- Address potential class imbalance using techniques like SMOTE or ADASYN.\")\n","print(\"- Perform more extensive hyperparameter tuning for both models using techniques like GridSearchCV or RandomizedSearchCV.\")\n","print(\"- Explore other advanced machine learning models (e.g., Gradient Boosting, Support Vector Machines).\")\n","print(\"- Investigate the impact of the newly engineered features on model performance.\")\n","print(\"- Consider feature selection techniques to identify the most relevant predictors and potentially simplify the models.\")\n","print(\"- Analyze the types of errors made by the models (false positives and false negatives) to understand the business implications and potentially adjust model thresholds.\")"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOimLPMSgjCdlySiFUHmj9v"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}